{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fe3e7dc-7e96-4d01-bf17-89387640ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['HF_HOME'] = '/root/autodl-tmp/huggingface_cache'\n",
    "# from intervented_model.llama import Intervented_LlamaForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import intervented_model.llama as llama\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import re\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class DataCollatorReward:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, data):\n",
    "        batch = {}\n",
    "        data_batch = []\n",
    "        for sample in data:\n",
    "            data_batch.append({\"input_ids\": sample['input_ids'], \"attention_mask\": sample[\"attention_mask\"]})\n",
    "        batch_data = self.tokenizer.pad(data_batch, padding=True, return_tensors=\"pt\")\n",
    "        batch['input_ids'] = batch_data['input_ids']\n",
    "        batch['attention_mask'] = batch_data['attention_mask']\n",
    "        return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa6fc4fe-53d4-4839-898e-e42c9c0d52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaModel, LlamaConfig, LlamaForCausalLM\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPast,\n",
    "    CausalLMOutputWithPast,\n",
    ")\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.utils.checkpoint\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "# from transformers.src.transformers.models.llama.modeling_llama import LlamaPreTrainedModel\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "class Intervented_LlamaModel(LlamaModel):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        value_model: Optional[nn.Module] = None,\n",
    "        lr: Optional[float] = None,\n",
    "        epochs: Optional[int] = None\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\n",
    "                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n",
    "            )\n",
    "\n",
    "        if self.gradient_checkpointing and self.training and use_cache:\n",
    "            logger.warning_once(\n",
    "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
    "            )\n",
    "            use_cache = False\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        past_seen_tokens = 0\n",
    "        if use_cache:  # kept for BC (cache positions)\n",
    "            if not isinstance(past_key_values, StaticCache):\n",
    "                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n",
    "                past_seen_tokens = past_key_values.get_seq_length()\n",
    "\n",
    "        if cache_position is None:\n",
    "            if isinstance(past_key_values, StaticCache):\n",
    "                raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "        print(\"DEBUG past_key_values type:\", type(past_key_values))\n",
    "        # causal_mask = self._update_causal_mask(\n",
    "        #     attention_mask, inputs_embeds, cache_position, past_seen_tokens + inputs_embeds.shape[1]\n",
    "        # )\n",
    "        print('hi i define causal mask')\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask,\n",
    "            inputs_embeds,\n",
    "            cache_position,\n",
    "            past_key_values,\n",
    "        )\n",
    "        # embed positions\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # decoder layers\n",
    "        print('hi here decoder layers')\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        next_decoder_cache = None\n",
    "        cos, sin = self.rotary_emb(hidden_states, position_ids)\n",
    "        position_embeddings = (cos, sin)\n",
    "        print('start decoder layer')\n",
    "        for decoder_layer in self.layers:\n",
    "            print('decoder layer, total ', len(self.layers))\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                    position_embeddings=position_embeddings\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            \n",
    "            if use_cache:\n",
    "                if isinstance(layer_outputs[-1], (tuple, list, Cache)):\n",
    "                    next_decoder_cache = layer_outputs[-1]\n",
    "                else:\n",
    "                    next_decoder_cache = None\n",
    "            else:\n",
    "                next_decoder_cache = None\n",
    "            print(\"DEBUG layer_outputs length:\", len(layer_outputs))\n",
    "\n",
    "            # if use_cache:\n",
    "            #     next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        \n",
    "        #### use the value model to optimize the hidden states\n",
    "\n",
    "        hidden_states_opt = hidden_states.detach().clone() # also need to set the hidden states to be requires_grad=True\n",
    "        hidden_states_opt.requires_grad = True\n",
    "\n",
    "        optimizer = torch.optim.SGD([hidden_states_opt], lr=lr)  # Using Adam optimizer to update x\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            for iteration in range(epochs):  # Run for a number of iterations\n",
    "                print('hi iter', iteration)\n",
    "                optimizer.zero_grad()   # Clear previous gradients\n",
    "                output = value_model(hidden_states_opt)         # Compute the function value at current x\n",
    "                loss = -output.sum()          # Negate the output to convert maximization problem to minimization\n",
    "                loss.backward()         # Compute gradients with respect to x\n",
    "                optimizer.step()        # Update x\n",
    "            \n",
    "        hidden_states = hidden_states_opt.detach().clone()  # Update the hidden states with the optimized x\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = None\n",
    "        if use_cache:\n",
    "            next_cache = (\n",
    "                next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n",
    "            )\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "\n",
    "class Intervented_LlamaForCausalLM(LlamaForCausalLM):\n",
    "    # def __init__(self, config, value_model: nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Intervented_LlamaModel(config)\n",
    "        self.value_model = None\n",
    "\n",
    "    def set_value_model(self, value_model):\n",
    "        self.value_model = value_model.to(torch.float16)\n",
    "    \n",
    "    def set_lr_and_epochs(self, lr, epochs):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "            value_model=self.value_model,\n",
    "            epochs = self.epochs,\n",
    "            lr = self.lr\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "            loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb14a025-5a51-4bb0-ba6f-371e49e18d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, default='vicuna_7B', choices=[\"vicuna_7B\", \"falcon_7B\", \"llama3_8B\"])\n",
    "parser.add_argument('--dataset_name', type=str, default='shp', choices=[\"hh_rlhf\", \"shp\"])\n",
    "parser.add_argument('--device', type=int, default=0)\n",
    "parser.add_argument('--use_intervention', default=True)\n",
    "parser.add_argument('--value_lr', default=0.0001)\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--lr', type=float, default=0.001)\n",
    "parser.add_argument('--bellman', default=False)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "if args.use_intervention:\n",
    "\n",
    "    value_model = ValueFunction(input_dim=4096, hidden_dim=4096, output_dim=1)\n",
    "    ##load weights\n",
    "    value_model.load_state_dict(torch.load(f'value_model_checkpoint/value_model_{args.model_name}_{args.dataset_name}.pth', \n",
    "                                           map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "MODEL_NAMES = { \n",
    "    'vicuna_7B': 'lmsys/vicuna-7b-v1.5', \n",
    "    'falcon_7B': 'tiiuae/falcon-7b-instruct',\n",
    "    'llama3_8B': 'meta-llama/Meta-Llama-3-8B'\n",
    "}\n",
    "\n",
    "DATASET_NAMES = { \n",
    "    'hh_rlhf': 'Anthropic/hh-rlhf', \n",
    "    'shp': 'stanfordnlp/SHP'\n",
    "}\n",
    "MODEL = MODEL_NAMES[args.model_name]\n",
    "DATASET = DATASET_NAMES[args.dataset_name]\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL, padding_side='left')\n",
    "\n",
    "device = f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6b7848e-72a4-462b-aba4-56ea4ee5bcf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183863c0a91e41ddab49f222e19d9b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee02f9d29ca4882a07cbe02ed507091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002c6c5c46b44120a9a8960e793ce697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc11baff3be940428168516bff463194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.use_intervention:\n",
    "    model = Intervented_LlamaForCausalLM.from_pretrained(MODEL, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=device, cache_dir='/root/autodl-tmp/huggingface_cache')\n",
    "    model.set_value_model(value_model)\n",
    "    model.set_lr_and_epochs(args.lr, args.epochs)\n",
    "else:\n",
    "    model = LlamaForCausalLM.from_pretrained(MODEL, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=device, cache_dir='/root/autodl-tmp/huggingface_cache')\n",
    "\n",
    "model.generation_config.temperature = None\n",
    "model.generation_config.top_k = None\n",
    "model.to(device)\n",
    "\n",
    "dataset = load_dataset(DATASET)\n",
    "\n",
    "if args.dataset_name == 'hh_rlhf':\n",
    "    dataset = dataset.remove_columns(\"rejected\")\n",
    "    for split in dataset.keys():\n",
    "        dataset[split] = dataset[split].rename_column('chosen', 'prompt')\n",
    "    test_dataset = dataset['test']\n",
    "elif args.dataset_name == 'shp':\n",
    "    test_file_path = 'dataset/test_dataset_shp.json'\n",
    "    test_dataset = load_dataset('json', data_files=test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "651fe40f-5190-46b0-a498-febeb99b6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.model_name == 'vicuna_7B':\n",
    "    begin_word = 'Human: '\n",
    "elif args.model_name == 'llama3_8B':\n",
    "    begin_word = 'User: '\n",
    "elif args.model_name == 'falcon_7B':\n",
    "    begin_word = 'User: '\n",
    "prompting = '''\n",
    "    A question from a curious user and an answer from an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the userâ€™s questions.\\n'''\n",
    "\n",
    "def preprocessing(example):\n",
    "    if args.dataset_name == 'hh_rlhf':\n",
    "        replaced_text = example['prompt'].replace(\"Human:\", begin_word)\n",
    "        parts = replaced_text.rsplit(\"Assistant:\", 1)  # Split the string at the last occurrence of \"Assistant:\"\n",
    "        result = parts[0] + \"Assistant:\"  # Append \"Assistant:\" back to the first part if needed\n",
    "    elif args.dataset_name == 'shp':\n",
    "        text = example['history']\n",
    "        result = begin_word + text + \"\\nAssistant:\"    \n",
    "\n",
    "\n",
    "    return {'prompt': result}\n",
    "\n",
    "\n",
    "# dataset = dataset.map(preprocessing)\n",
    "test_dataset = test_dataset.map(preprocessing)\n",
    "dataloader = DataLoader(test_dataset['train'], batch_size=1) # only use the test set for now\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a06ea8-b02e-40e5-8b19-1dd2428588b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG past_key_values type: <class 'transformers.cache_utils.DynamicCache'>\n",
      "hi i define causal mask\n",
      "hi here decoder layers\n",
      "start decoder layer\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "decoder layer, total  32\n",
      "DEBUG layer_outputs length: 1\n",
      "hi iter 0\n",
      "hi iter 1\n",
      "hi iter 2\n",
      "hi iter 3\n",
      "hi iter 4\n",
      "hi iter 5\n",
      "hi iter 6\n",
      "hi iter 7\n",
      "hi iter 8\n",
      "hi iter 9\n",
      "hi iter 10\n",
      "hi iter 11\n",
      "hi iter 12\n",
      "hi iter 13\n",
      "hi iter 14\n",
      "hi iter 15\n",
      "hi iter 16\n"
     ]
    }
   ],
   "source": [
    "generated_responses = []\n",
    "for batch_prompts in tqdm(dataloader):\n",
    "    # print(batch_prompts)\n",
    "    encoded_inputs = tokenizer(batch_prompts['prompt'], return_tensors=\"pt\", padding=True) # tokenize the prompt\n",
    "    input_ids = encoded_inputs['input_ids'].to(device)\n",
    "    attention_mask = encoded_inputs['attention_mask'].to(device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=128, num_return_sequences=1, return_dict_in_generate=True)\n",
    "    outputs_text = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    for prompt, output in zip(batch_prompts['prompt'], outputs_text):\n",
    "        generated_responses.append({'prompt': prompt, 'result': output.removeprefix(prompt), 'response': output})\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f982c4-360f-48e1-bd52-a2973c1ea681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
